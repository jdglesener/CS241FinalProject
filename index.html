<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<meta property="og:image" content="lm3.png" />

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

	<title>Data for Data</title>
	
	<style type="text/css" media="screen">
	
		body {
			line-height: 140%;
			margin: auto;
			width: 800px;
		}
		code {font-size: 120%;}
		
		
		pre code {
			background-color: #FFC7C7;
			color: #000000;
			
			display: block;
			padding: 20px;
		}
		
		.centered {
			position: fixed;
			top: 50%;
			left: 50%;
			transform: translate(-50%, -50%);
}
		
	</style>
	
</head>

<body bgcolor="000000" text="7D1616">

<style>
 body {
        counter-reset: h1counter;
    }
    h1 {
        counter-reset: h2counter;
    }
    h2 {
        counter-reset: h3counter;
    }
    h2:before {
        content: counter(h2counter) ".\0000a0\0000a0";
        counter-increment: h2counter;
    }
    h3:before {
        content: counter(h2counter) "." counter(h3counter) ".\0000a0\0000a0";
        counter-increment: h3counter;
    }
</style>
	
<h1 id="preamble">Data Structures for Data Analysis for Data Visualization</h1>

<p>I wanted to analyze data on election donations in the 2016 and 2020 presidential cycles to find how donors shifted between candidates in those two cycles. To do so, I looked at all cases in which there was a maximum individual donation under the same name in 2016 and 2020 and then plotting them in a heatmap.</p>

<p>See the source code <a href="https://github.com/jdglesener/CS241FinalProject">here</a>

<center>
<img src="lm3.png" height=240px>
</center>

<h2 >The Data</h2>

The data is taken from Kaggle and is about the Geeks for Geeks youtube channel, but the actual data itself is not super relevant to the analysis done in this project. There are 2000 observations of 9 variables. If you did want to do some analysis however, I figured there could possibly be a relationship between the duration of the video and the number of the views. There also needs to be some key so that each video is unique. 
So the getkey() method takes a line of the csv and returns the relevant information as a list to be stored into a data structures
You can look at the data <a href="https://www.kaggle.com/datasets/ashishjangra27/geeksforgeeks-youtube">here</a>
<pre><code># get key value from line
def get_key(l):
    views = ""
    title = ""
    duration = ""
    commas = 0
    quotes = False
    for c in l:
        if c == '\"':
            quotes = not quotes
        if commas == 0:
            duration += c
        if commas == 1:
            title += c
        if commas == 2:
            views += c
        if c == ',' and not quotes:
            commas += 1    
    return [title[:-1], views[:-1], duration[:-1]]</code></pre>
The relevant data is the title of the video, the duration of the video, and the number of views it got to create a potential views v duration scatterplot.
<h2 >The Structure</h2>

<p>The data set was approximately 100 megabytes in size encoded as a .csv files. I read the files line by line in a binary search tree that stored the donor name as a key value and then stored as values the donee in two sets, one for 2016 and one for 2020. I used <code>get_kv()</code> as my function to read lines.</p>

<p>This is where you would use the library of the week if you wanted to use a library.</p>

<pre><code># a bst is a list - less, more, key, vals
# key is a donor
# vals is a year indexed list of sets of donees

# file line to bst entry
def insert(data, tree, func, year):
	# use func to read a line of data from file
	key, val = func(data)
	# traverse
	while tree and key != tree[2]:
		tree = tree[key > tree[2]]
	if tree:
	# add value to key
		tree[3][year].add(val)
	elif not year:
	# add key to tree
		tree.extend([[],[], key, [{val},set()]])</code></pre>
<h2 >The Analysis</h2>

I put the lists from each line into 3 separate data structures, the Binary Search Tree class from our second homework assignment, the built-in dictionary, and the Hashing dictionary class that is written here. 
<pre><code>class LibTable():
    def __init__(self, key = None, val = None):
        self.size = 5000
        self.table = [[] for i in range(self.size)]
        self.keys = [key]
        if val:
            self.table[hash(key)%self.size].append((val,key))
    
    def insert(self, key, val):
        self.keys.append(key)
        if type(key) != list:
            self.table[hash(key)%self.size].append((val,key))
        else:
            self.table[hash(key[0])%self.size].append((val,key))
    
    def get(self, key):
        if type(key) != list:
            ind = hash(key)%self.size
        else:
            ind = hash(key[0])%self.size
        if key in self.keys:
            for i in self.table[ind]:
                if key in i:
                    return i[0]

    def get_keys(self):
        return self.keys

    
    def __repr__(self):
        return str(self.table)
    
    def __str__(self):
        return self.__repr__()</pre></code>

Once I had these dictionaries on hand, a few shortcomings emerged. First, the data was not sorted in anyway. Bernie has far more in common with Hillary and Trump than with Santorum and McAfee visually, mainly due to the sheer quantity of donors. So I ordered donees along both axis by number of donors. This also showed some trailing candidates only shared donors with eventual nominees. I choose to take these candidates, such as McMullin, out of the dataset.

<pre><code>noms = ["HILLARY", "DONALD J TRUMP", "BIDEN"]

# filter 20s candidates who only received co-donations with nominees
innr = [i for i in d[next(iter(d))] if sum([d[o][i] for o in d if o not in noms])]

# rank 20s candidates donor counts
innr = sorted(innr, key = lambda i : sum([d[o][i] for o in d]))[::-1]

# filter 16s candidates who only received co-donations with nominees
outr = [o for o in d if sum([d[o][i] for i in innr if i not in noms])]

# sort inner and outer dictionaries
d = { o : { innr[i] : d[o][innr[i]] for i in range(len(innr)) } for o in outr }
d = dict(sorted(list(d.items()), key = lambda x : sum(x[1].values()))[::-1])</pre></code>

<h2 >The Visualization</h2>

I don't really know how Seaborn works, so I just ran it with default settings and looked up things that annoyed me. I wanted a transparent background and nice color theme in the graph to display with the website HTML theme, had to make it high resolution so I could read the (many) labels, and copied a script from Stack Overflow that manipulated tick marks to move labels from the bottom to the top of the x axis. Otherwise, I simply ran a heatmap over the dictionary and presented what I found!

<pre><code>import pandas
import seaborn
import matplotlib.pyplot as plt

df = pandas.DataFrame.from_dict(d)

plt.rcParams['figure.figsize'] = [10, 10]
plt.rcParams['figure.dpi'] = 100
plt.tick_params(axis='both', which='major', labelsize=10, labelbottom = False, 
                bottom=False, top = False, left = False, labeltop=True)
plt.style.use("dark_background")

p = seaborn.heatmap(df, cmap=seaborn.color_palette("mako", as_cmap=True))
p.get_figure().savefig("heat_final.png",bbox_inches='tight',transparent=True)</pre></code>

<p>I did touch up a bit with Glimpse at the end. I converted the black font via color exchange to match the HTML text color, and placed the image on a black background for distribution outside of the host page. I also clipped the tick marks from the legend as they were unhelpfully quantitative.</p><br><br>

<center>
<img src="heat_final.png" height=640px>
</center>

</body>
</html>
